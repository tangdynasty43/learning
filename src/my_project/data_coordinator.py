"""æ•°æ®åè°ƒå™¨ - ç®¡ç†å¤šä¸ªæ–‡ä»¶é—´çš„æ•°æ®æµå’Œæ‰§è¡Œåè°ƒ
è¿™ä¸ªæ–‡ä»¶å±•ç¤ºå¦‚ä½•åè°ƒå¤šä¸ªPythonæ–‡ä»¶çš„æ‰§è¡Œå’Œæ•°æ®äº¤æ¢ã€‚
"""

import time
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from data_manager import get_data_manager, save_data, load_data, list_data

class DataCoordinator:
    """æ•°æ®åè°ƒå™¨ï¼Œç®¡ç†æ–‡ä»¶é—´çš„æ•°æ®æµå’Œæ‰§è¡Œé¡ºåºã€‚"""
    
    def __init__(self):
        self.dm = get_data_manager()
        self.execution_log = []
        self.start_time = datetime.now()
        
        print("ğŸ¯ æ•°æ®åè°ƒå™¨å¯åŠ¨")
        self.log_event("coordinator_started", "æ•°æ®åè°ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def log_event(self, event_type: str, message: str, data: dict = None):
        """è®°å½•æ‰§è¡Œäº‹ä»¶ã€‚"""
        event = {
            "timestamp": datetime.now().isoformat(),
            "event_type": event_type,
            "message": message,
            "data": data or {}
        }
        
        self.execution_log.append(event)
        print(f"ğŸ“ [{event_type}] {message}")
        
        # ä¿å­˜åˆ°å…±äº«æ•°æ®
        self.dm.save_shared_data("execution_log", self.execution_log)
    
    def clear_shared_data(self):
        """æ¸…ç†æ‰€æœ‰å…±äº«æ•°æ®ï¼Œé‡æ–°å¼€å§‹ã€‚"""
        print("\nğŸ§¹ æ¸…ç†å…±äº«æ•°æ®...")
        
        all_keys = self.dm.list_shared_data()
        cleared_count = 0
        
        for key in all_keys:
            if key != "execution_log":  # ä¿ç•™æ‰§è¡Œæ—¥å¿—
                if self.dm.delete_shared_data(key):
                    cleared_count += 1
        
        self.log_event("data_cleared", f"å·²æ¸…ç† {cleared_count} ä¸ªå…±äº«æ•°æ®æ–‡ä»¶")
        return cleared_count
    
    def run_python_file(self, file_path: str, wait_for_completion: bool = True):
        """è¿è¡ŒPythonæ–‡ä»¶ã€‚
        
        Args:
            file_path: Pythonæ–‡ä»¶è·¯å¾„
            wait_for_completion: æ˜¯å¦ç­‰å¾…æ‰§è¡Œå®Œæˆ
        """
        file_name = Path(file_path).name
        self.log_event("file_execution_start", f"å¼€å§‹æ‰§è¡Œ {file_name}")
        
        try:
            if wait_for_completion:
                # åŒæ­¥æ‰§è¡Œ
                result = subprocess.run(
                    [sys.executable, file_path],
                    capture_output=True,
                    text=True,
                    cwd=Path(file_path).parent
                )
                
                if result.returncode == 0:
                    self.log_event("file_execution_success", f"{file_name} æ‰§è¡ŒæˆåŠŸ", {
                        "stdout": result.stdout,
                        "execution_time": (datetime.now() - self.start_time).total_seconds()
                    })
                    return True
                else:
                    self.log_event("file_execution_error", f"{file_name} æ‰§è¡Œå¤±è´¥", {
                        "stderr": result.stderr,
                        "returncode": result.returncode
                    })
                    return False
            else:
                # å¼‚æ­¥æ‰§è¡Œ
                process = subprocess.Popen(
                    [sys.executable, file_path],
                    cwd=Path(file_path).parent
                )
                self.log_event("file_execution_async", f"{file_name} å¼‚æ­¥æ‰§è¡Œå¯åŠ¨", {
                    "pid": process.pid
                })
                return process
                
        except Exception as e:
            self.log_event("file_execution_exception", f"{file_name} æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return False
    
    def wait_for_data(self, key: str, timeout: int = 60, check_interval: int = 2):
        """ç­‰å¾…ç‰¹å®šæ•°æ®å‡ºç°ã€‚"""
        self.log_event("wait_for_data_start", f"ç­‰å¾…æ•°æ® '{key}'")
        
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            data = self.dm.load_shared_data(key)
            if data is not None:
                self.log_event("wait_for_data_success", f"æ•°æ® '{key}' å·²å¯ç”¨")
                return data
            
            time.sleep(check_interval)
        
        self.log_event("wait_for_data_timeout", f"ç­‰å¾…æ•°æ® '{key}' è¶…æ—¶")
        return None
    
    def check_data_dependencies(self, required_keys: list):
        """æ£€æŸ¥æ•°æ®ä¾èµ–æ˜¯å¦æ»¡è¶³ã€‚"""
        missing_keys = []
        available_keys = []
        
        for key in required_keys:
            data = self.dm.load_shared_data(key)
            if data is not None:
                available_keys.append(key)
            else:
                missing_keys.append(key)
        
        self.log_event("dependency_check", "æ•°æ®ä¾èµ–æ£€æŸ¥å®Œæˆ", {
            "required": required_keys,
            "available": available_keys,
            "missing": missing_keys
        })
        
        return len(missing_keys) == 0, missing_keys
    
    def orchestrate_data_pipeline(self):
        """åè°ƒæ•´ä¸ªæ•°æ®å¤„ç†æµæ°´çº¿ã€‚"""
        print("\nğŸš€ å¼€å§‹åè°ƒæ•°æ®å¤„ç†æµæ°´çº¿")
        
        # ç¬¬ä¸€æ­¥ï¼šæ¸…ç†æ—§æ•°æ®
        self.clear_shared_data()
        
        # ç¬¬äºŒæ­¥ï¼šè¿è¡Œæ•°æ®ç”Ÿäº§è€…
        producer_path = Path(__file__).parent / "file_a_producer.py"
        if producer_path.exists():
            print("\nğŸ“Š æ­¥éª¤1: è¿è¡Œæ•°æ®ç”Ÿäº§è€…")
            success = self.run_python_file(str(producer_path))
            
            if not success:
                self.log_event("pipeline_error", "æ•°æ®ç”Ÿäº§è€…æ‰§è¡Œå¤±è´¥ï¼Œåœæ­¢æµæ°´çº¿")
                return False
        else:
            self.log_event("pipeline_error", f"æ‰¾ä¸åˆ°æ•°æ®ç”Ÿäº§è€…æ–‡ä»¶: {producer_path}")
            return False
        
        # ç¬¬ä¸‰æ­¥ï¼šç­‰å¾…å…³é”®æ•°æ®
        print("\nâ³ æ­¥éª¤2: ç­‰å¾…å…³é”®æ•°æ®ç”Ÿæˆ")
        required_data = ["excel_processing_result", "users", "projects"]
        
        for key in required_data:
            data = self.wait_for_data(key, timeout=30)
            if data is None:
                self.log_event("pipeline_error", f"å…³é”®æ•°æ® '{key}' æœªèƒ½åŠæ—¶ç”Ÿæˆ")
                return False
        
        # ç¬¬å››æ­¥ï¼šè¿è¡Œæ•°æ®æ¶ˆè´¹è€…
        consumer_path = Path(__file__).parent / "file_b_consumer.py"
        if consumer_path.exists():
            print("\nğŸ“ˆ æ­¥éª¤3: è¿è¡Œæ•°æ®æ¶ˆè´¹è€…")
            success = self.run_python_file(str(consumer_path))
            
            if not success:
                self.log_event("pipeline_error", "æ•°æ®æ¶ˆè´¹è€…æ‰§è¡Œå¤±è´¥")
                return False
        else:
            self.log_event("pipeline_error", f"æ‰¾ä¸åˆ°æ•°æ®æ¶ˆè´¹è€…æ–‡ä»¶: {consumer_path}")
            return False
        
        # ç¬¬äº”æ­¥ï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        print("\nğŸ“‹ æ­¥éª¤4: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
        self.generate_final_report()
        
        self.log_event("pipeline_success", "æ•°æ®å¤„ç†æµæ°´çº¿æ‰§è¡Œå®Œæˆ")
        return True
    
    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆçš„æ‰§è¡ŒæŠ¥å‘Šã€‚"""
        all_data = self.dm.list_shared_data()
        
        report = {
            "pipeline_execution": {
                "start_time": self.start_time.isoformat(),
                "end_time": datetime.now().isoformat(),
                "duration_seconds": (datetime.now() - self.start_time).total_seconds(),
                "status": "completed"
            },
            "data_summary": {
                "total_data_files": len(all_data),
                "data_files": all_data
            },
            "execution_events": len(self.execution_log),
            "recommendations": []
        }
        
        # æ£€æŸ¥å„ä¸ªç»„ä»¶çš„æ‰§è¡Œç»“æœ
        excel_result = self.dm.load_shared_data("excel_processing_result")
        if excel_result:
            if excel_result.get('status') == 'success':
                report["recommendations"].append("âœ… Excelæ•°æ®å¤„ç†æˆåŠŸ")
            else:
                report["recommendations"].append("âŒ Excelæ•°æ®å¤„ç†å¤±è´¥")
        
        consumer_status = self.dm.load_shared_data("consumer_status")
        if consumer_status:
            report["recommendations"].append("âœ… æ•°æ®æ¶ˆè´¹è€…æ‰§è¡Œå®Œæˆ")
        
        analysis_report = self.dm.load_shared_data("analysis_report")
        if analysis_report:
            report["recommendations"].append("âœ… æ•°æ®åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ")
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        self.dm.save_shared_data("final_pipeline_report", report)
        
        print("\nğŸ“Š === æœ€ç»ˆæ‰§è¡ŒæŠ¥å‘Š ===")
        print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {report['pipeline_execution']['duration_seconds']:.2f} ç§’")
        print(f"ğŸ“ ç”Ÿæˆæ•°æ®æ–‡ä»¶: {report['data_summary']['total_data_files']} ä¸ª")
        print(f"ğŸ“ æ‰§è¡Œäº‹ä»¶: {report['execution_events']} ä¸ª")
        print("\nğŸ¯ æ‰§è¡Œç»“æœ:")
        for rec in report["recommendations"]:
            print(f"  {rec}")
        
        return report
    
    def monitor_data_changes(self, duration: int = 60):
        """ç›‘æ§å…±äº«æ•°æ®çš„å˜åŒ–ã€‚
        
        Args:
            duration: ç›‘æ§æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        """
        print(f"\nğŸ‘ï¸ å¼€å§‹ç›‘æ§æ•°æ®å˜åŒ– ({duration}ç§’)")
        
        initial_data = set(self.dm.list_shared_data())
        start_time = time.time()
        
        while time.time() - start_time < duration:
            current_data = set(self.dm.list_shared_data())
            
            # æ£€æŸ¥æ–°å¢çš„æ•°æ®
            new_data = current_data - initial_data
            if new_data:
                for key in new_data:
                    self.log_event("data_added", f"æ–°å¢æ•°æ®: {key}")
                initial_data = current_data
            
            time.sleep(2)
        
        print("ğŸ‘ï¸ ç›‘æ§ç»“æŸ")

def run_interactive_demo():
    """è¿è¡Œäº¤äº’å¼æ¼”ç¤ºã€‚"""
    coordinator = DataCoordinator()
    
    while True:
        print("\n" + "="*50)
        print("ğŸ¯ æ•°æ®åè°ƒå™¨ - äº¤äº’å¼æ¼”ç¤º")
        print("="*50)
        print("1. è¿è¡Œå®Œæ•´æ•°æ®æµæ°´çº¿")
        print("2. æ¸…ç†æ‰€æœ‰å…±äº«æ•°æ®")
        print("3. æŸ¥çœ‹å½“å‰å…±äº«æ•°æ®")
        print("4. è¿è¡Œæ•°æ®ç”Ÿäº§è€…")
        print("5. è¿è¡Œæ•°æ®æ¶ˆè´¹è€…")
        print("6. ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š")
        print("7. ç›‘æ§æ•°æ®å˜åŒ–")
        print("0. é€€å‡º")
        
        choice = input("\nè¯·é€‰æ‹©æ“ä½œ (0-7): ").strip()
        
        if choice == "1":
            coordinator.orchestrate_data_pipeline()
        elif choice == "2":
            coordinator.clear_shared_data()
        elif choice == "3":
            data_list = coordinator.dm.list_shared_data()
            print(f"\nğŸ“ å½“å‰å…±äº«æ•°æ® ({len(data_list)} ä¸ª):")
            for key in data_list:
                info = coordinator.dm.get_data_info(key)
                size = info.get('size_bytes', 0) / 1024 if info else 0
                print(f"  ğŸ“„ {key}: {size:.2f} KB")
        elif choice == "4":
            producer_path = Path(__file__).parent / "file_a_producer.py"
            coordinator.run_python_file(str(producer_path))
        elif choice == "5":
            consumer_path = Path(__file__).parent / "file_b_consumer.py"
            coordinator.run_python_file(str(consumer_path))
        elif choice == "6":
            coordinator.generate_final_report()
        elif choice == "7":
            duration = input("ç›‘æ§æ—¶é•¿ï¼ˆç§’ï¼Œé»˜è®¤60ï¼‰: ").strip()
            duration = int(duration) if duration.isdigit() else 60
            coordinator.monitor_data_changes(duration)
        elif choice == "0":
            print("ğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")

if __name__ == "__main__":
    print("ğŸ¯ æ•°æ®åè°ƒå™¨å¯åŠ¨")
    print("é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. è‡ªåŠ¨è¿è¡Œå®Œæ•´æµæ°´çº¿")
    print("2. äº¤äº’å¼æ¼”ç¤º")
    
    mode = input("è¯·é€‰æ‹© (1-2): ").strip()
    
    if mode == "1":
        coordinator = DataCoordinator()
        success = coordinator.orchestrate_data_pipeline()
        if success:
            print("\nğŸ‰ æµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
        else:
            print("\nâŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥ï¼")
    elif mode == "2":
        run_interactive_demo()
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")